---
title: "NYC Council Data Science Test - ANALYSIS"
author: "Caffrey Lee"
date: "`r Sys.Date()`"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, comment = F)
```

```{r libraries, include=FALSE}
library(prettydoc)
library(data.table)
library(DT)
library(ggplot2)
library(scales)
library(xts)
library(zoo)
library(forecast)
library(tseries)
library(formulaic)
#library(ggseas)
library(ggthemes)
library(ggmap)
library(stddiff)
```

```{r load data}
dat = fread("NYPD_Arrests_Data__Historic_.csv")
pop = fread('New_York_City_Population_by_Borough__1950_-_2040.csv') # this is modified version and use the uniformed total population of 2010 for across the year. 
setnames(pop, old = c('V1', 'V2'), new = c('arrest_boro', '2010_ pop'))
pop[, total:= sum(`2010_ pop`)]

```

```{r functions}

basic.stat <- function(dat){
  print(hist(dat))
  quantile(dat)
}

length.unique <- function(dat, value.name, by = NA){
  require(data.table)
  this.dat <- setDT(x = dat)
  
  if(length(by) > 0){
    if(!is.na(by[1])){
      if(by[1] == ""){
        by[1] <- NA
      }
    }
  }
  
  if(is.na(by[1])){
    dat.u <- unique(this.dat, by = value.name)
    ans <- dat.u[, .(N = .N)]
  }
  if(!is.na(by[1])){
    dat.u <- unique(this.dat, by = c(by, value.name))
    ans <- dat.u[, .N, by = by]
  }
  return(ans)
}


round.numerics <- function(x, digits = 3){
  if(is.numeric(x)){
    x <- round(x = x, digits = digits)
  }
  return(x)
}

create.histogram <- function(dat, title, show.labels = T)
{
  if(show.labels == T){
    ggplot(data = dat, aes(x = yr, y = perc)) +
    geom_bar(stat = 'identity', aes(fill = target), colour = 'black', width = 0.6)+
    facet_wrap(~ target) +
    scale_fill_brewer(palette = "Spectral") +
    geom_text(aes(label = paste0(perc*100," %")), vjust = -0.5,  size = 4) +
    theme(legend.position = "top") +
    xlab("Year") +
    ylab("The rates") +
    ggtitle(sprintf("Arrest Rates by %s", title))
  } else {
    ggplot(data = dat, aes(x = yr, y = perc)) +
    geom_bar(stat = 'identity', aes(fill = target), colour = 'black', width = 0.6)+
    facet_wrap(~ target) +
    scale_fill_brewer(palette = "Spectral") +
    theme(legend.position = "top") +
    xlab("Year") +
    ylab("The rates") +
    ggtitle(sprintf("Arrest Rates by %s", title))
  }
}

create.slop.chart <- function(dat, title){
  theme_set(theme_classic())
  
  dat.2015 = dat[yr == min(dat$yr),] 
  dat.2018 = dat[yr == max(dat$yr),]
  dat.condensed = merge(dat.2015[, c(2,5)], dat.2018[, c(2,5)], by = 'target')
  setnames(dat.condensed, old = c('perc.x', 'perc.y'), new = c(as.character(min(dat$yr)), as.character(max(dat$yr))))
  
  left= paste(min(dat$yr), dat.condensed$target,paste0( (dat.condensed$`2015`)*100, " %"), sep= ', ')
  right= paste(max(dat$yr), dat.condensed$target,paste0( (dat.condensed$`2018`)*100, " %"), sep= ', ')

  dat.condensed$class = ifelse((dat.condensed$`2015` - dat.condensed$`2018`) < 0, 'green', 'red')


  plot = 
    ggplot(dat.condensed) + geom_segment(aes(x=1, xend=2, y= `2015`, yend=`2018`, col= class), size=.75, show.legend=F) + 
                  geom_vline(xintercept=1, linetype="dashed", size=.1) + 
                  geom_vline(xintercept=2, linetype="dashed", size=.1) +
                  scale_color_manual(labels = c("Up", "Down"), 
                                     values = c("green"="#00ba38", "red"="#f8766d")) +  
                  labs(x="", y=sprintf("Rate by %s", title)) + 
    xlim(.5, 2.5)

  
  # Add texts
  plot = plot + geom_text(label=left, y= dat.condensed$`2015`, x=rep(1, NROW(dat.condensed)), hjust=1.1, size=3.5)
  plot = plot + geom_text(label=right, y= dat.condensed$`2018`, x=rep(2, NROW(dat.condensed)), hjust=-.1, size=3.5)

  plot = plot + geom_text(label="Beginning", x=1, y=1.1*(max(dat.condensed$`2015`, dat.condensed$`2018`)), hjust=1.2, size=3) 
  
  plot = plot + geom_text(label="Ending", x=1, y=1.1*(max(dat.condensed$`2015`, dat.condensed$`2018`)), hjust=1.2, size=3) 
  
  plot + theme(panel.background = element_blank(), 
           panel.grid = element_blank(),
           axis.ticks = element_blank(),
           axis.text.x = element_blank(),
           panel.border = element_blank(),
           plot.margin = unit(c(1,2,1,2), "cm"))
  print(plot)
}

ssacf <- function(x){
  sum(acf(x, na.action = na.omit)$acf^2)
}# reference: https://www.r-bloggers.com/is-my-time-series-additive-or-multiplicative/


compare_ssacf <-function(add,mult){
  ifelse(ssacf(add) < ssacf(mult), "Additive", "Multiplicative")
}
# reference: https://www.r-bloggers.com/is-my-time-series-additive-or-multiplicative/

my.format <- function(x, big.mark = ","){
  return(format(x = x, big.mark = big.mark))
}
```



### Introduction: 

*The purpose of this document is for the NYC Council Data Science Interview. It is done by Caffrey Lee solely.*


### Description of the data: {.tabset}
#### Given Data :

The given data contains the individual record of the arrest with the following number of observations and the number of features:  `r nrow(dat)` number of observations and `r ncol(dat)`features. Table 1 provides a glimpse of it, the first 5 rows.
```{r data}
dat$ARREST_DATE = as.Date(dat$ARREST_DATE, format = "%m/%d/%Y")
datatable(head(dat,5), caption = 'Table 1. Given data')
```

#### Population (NYC OPEN DATA):

Table 2. shows the 2010 population of each borough and the total population of that year. **Please aware that this is manually modified for the analysis**. (source: https://data.cityofnewyork.us/City-Government/New-York-City-Population-By-Community-Districts/xi7c-iiu2))

```{r pop data}
datatable(pop, caption = 'Table 2. 
          (NOTE: This is modified manually for the analysis)')
```

### Exploring the data:

After exploring the data I found some issues with the data that I need to clean before the analysis: 

  * *arrest_date is not in a date format; *
  * *there are missing values;* 
  * *some of the unique keys of the categorical variables are not descriptive;*
  * *too many categories for a single feature;*
  
### Data Preperation: {.tabset}

After cleaning the data, the final data that I utilized for the analysis has 4789450 of observations with 20 columns with a missing rate of 0%. In short, the following analysis is based on the *99.81%* of the given data.

#### Imputing missing values 

Looking at Table 3, one could see that the entire columns are missing. That is, it is impossible to trace back and impute the values. Also, the missing rates are not critical: **0.19%**. Hence, I decided to simply remove the rows that are missing by dropping the NA values.
```{r missing values}
w = which(is.na(dat))
datatable(dat[w,][1:5], caption = 'Table 3: This is missing value table', options = list(columnDefs = list(list(
  targets = 1,
  render = JS("function(data, type, row, meta) {",
    "return data === null ? 'NA' : data;","}")))))
# it seems that every columns are missing as well. In other words, no clue to trace back! Just drop them. 

clean_dat = na.omit(dat)
```

#### AREREST_BORO 
* *AREREST_BORO* has 6 unique factors: B, K, M, Q, S, and " ". 
" " were scattered across the boroughs; there were only 8 cases. So, I simply dropped them. 
```{r unique values check}
## before that make the columsn in to low key... 
colnames(clean_dat) = tolower(colnames(clean_dat))

datatable(clean_dat[, .N, by = arrest_boro])

## using long and lat to find out: 
geo.tab = clean_dat[, arrest_boro, by = c('longitude', 'latitude')]
map =  get_map(c(left = min(geo.tab[arrest_boro == "", longitude]), bottom = min(geo.tab[arrest_boro == "", latitude]), right = max(geo.tab[arrest_boro == "", longitude]), top = max(geo.tab[arrest_boro == "", latitude])))

ggmap(map) + geom_point(data=geo.tab[arrest_boro == ""], aes(x=longitude, y=latitude, color=arrest_boro))

rm(geo.tab)
clean_dat = clean_dat[arrest_boro != "", ]


```


#### PERP_RACE
* *PERP_RACE* has 8 categories with UNKNOWN AND OTHERS. I prefer not to have unknown, especially when there is a category called "Other". So, I put them together. (But, this may not be a good idea) 

```{r race}
pop.tab = clean_dat[, .N, by = perp_race]
pop.tab[, perc := round(N/sum(N), digits = 2)]
datatable(pop.tab, caption = 'Table 4.')
clean_dat[perp_race == 'UNKNOWN', perp_race := 'OTHER']
```

#### AGE_GROUP
* *AGE_GROUP* has 2000, 378, and other numbers that are not descriptive at all. Those non-descriptive numbers were recategorized into **UNKNOWN**

```{r age_group}
#### NOW clean the age group
# I will put anything that is not in the range, pure numerical numbers into the UNKNOWN. 
clean.group = c('<18','18-24','25-44','45-64','65+','UNKNOWN')
clean_dat[! age_group %in% clean.group, age_group := 'UNKNOWN']

datatable(clean_dat[, .N, by = age_group], caption = 'Table 5.')
```

#### New Columns
* *yr* and *yr_mon* were introduced for the analysis 
```{r yr}
# creating yr and yr_mon columns for later
clean_dat[, yr:= as.numeric(format(arrest_date, '%Y'))]
clean_dat[, yr_mon:= format(as.Date(clean_dat$arrest_date, format = '%Y-%m-%d'), format ='%Y-%m')]
datatable(clean_dat[, .N, by = yr], caption = 'Table 6.')
datatable(clean_dat[, .N, by = yr_mon], caption = 'Table 7.')
```

### ANALYSIS & ANSWERING: 

#### **1. Has the arrest rate been decreasing from 2015-2018? Describe the trend and defend any statistical tests used to support this conclusion.** {.tabset}

*Population* would be one of the most commonly used candidates to compute the *arrest rate* as followed:  $$ Arrest / Population $$ 
where Arrest represents the number of arrests and Population is total population **(uniformed)**. 

Yet, directly using the total population has *shortcomings*, for instance, the uniformed data may mislead the result since it does *not* take the demographic differences among the neighbors into considerations such as ethnicity, social environment, economic status, and such. 
In the question, the definition of the arrest rate is a bit abstract. The rate could be based on the population or other variables such as race, age, borough, sex, arrest types, and such. Hence, I would also utilize the above-mentioned variables to calculate the arrest rate of each case by year as well as the total population. 

Another thing that I would like to mention is that it is not clear if the difference is between the two years, 2015 and 2018 only, or the overall trend across time. To be safe, I will plot the slop chart for the difference between 2015 and 2018; and, I will plot the histogram (bar) to show the trend during the time. 
Before I jumped into the analysis, I would like to make the following assumption: 

+ Assumptions:
  - I assume that the population does *not* fluctuate across time. (This is confirmed with the 2018 NYC population data from Statista: the total population is 8,398,748 when `r my.format(8242624)` is the total population that I used across the year; reference: https://www.statista.com/statistics/799563/resident-population-of-nyc-by-race/)
  - For simplicity, I *ignore* the traits of the neighbors such as the proportion of each ethnicity, social environment, economic status, and such.
  - I presume that every case is *independent*. This also excludes multiple crime, arrest, cases of a single person. 
  - Consecutive observations are *equally spaced*
  - Apply a *discrete-time* index, yearly, (even though this may only hold approximately) 


For descriptive statistics, trend analysis on the overall arrest rate, arrest rate difference between 2015 and 2018, and the arrest rate of demographics - race, age group, borough, and gender - and top 5 arrest types.
 

##### Trend Analysis 

+ Methodology: Calculate the **arrest rate**, calculated by `number of arrests divided by the total population`, of each month and compare that to each other throughout the year as well as taking a close look at the monthly data.

Based on time series analysis on the overall arrest rate, one could say that the arrest rate has a **decreasing trend** with seasonality. (Note: the number of arrests has been reported in the data also decreases as the time goes)

To have a better understanding of the seasonality, I plotted out the sub seasonality plot; and, it aligns with the Seasonal Trend Decomposition using Loess (STL). Broadly speaking, the arrest rates fluctuate throughout the years; and December has the lowest arrest rate when January has the lowest variation.  The lowest arrest rate in December may be explained by the Holiday season.
```{r question 1 time series}
rm(dat)
total.pop = unique(pop$total)
# making sure that dates are ascending order 
setorderv(x = clean_dat, cols = 'arrest_date', order = 1)

arrest.dat = clean_dat[, .N, by = c('yr_mon')]
arrest.dat = arrest.dat[, .(Rate = (N/total.pop)*100), by = yr_mon]

# creating the ts dat
ts.dat = ts(data = arrest.dat[,-1], start = c(2006,1), end = c(2018,12), frequency = 12)

# parsing only 2015 to 2018
ts.dat.window = window(ts.dat, 2015, c(2018,12))

#time(ts.dat.window)
#cycle(ts.dat.window)
#cor(ts.dat.window, lag(ts.dat.window), use = 'complete.obs')

autoplot(ts.dat.window, facet = T)+ ylab('Overall Arrest Rate')

# STL (Seasonality and Trend Decomposition using Loess)
autoplot(stl(ts.dat.window[,1], s.window = 'periodic', robust = T))

ggAcf(ts.dat.window, lag.max = 12*4)
pacf(ts.dat.window)

# Chekcing for the seaonality
ggseasonplot(ts.dat.window, year.labels = TRUE, year.labels.left = TRUE) +
  ylab("Arrest Rate") +
  xlab("Month") +
  ggtitle("Seasonal Plot: Overall Arrest Rate") # Weak Seaonality

# Variation by the month
ggsubseriesplot(ts.dat.window) +
  ylab("Arrest Rate") +
  xlab("Month") +
  ggtitle("Sub Seasonal Plot: Overall Arrest Rate")


boxplot(ts.dat.window~cycle(ts.dat.window), xlab = 'Months', ylab = 'Overall Arrest Rate')
```


##### Difference Between 2015 & 2018
+ Methodology: Calculating the difference between the arrest rates of only those two years: 2015 and 2018. Then, plot out the slope chart to show the relationship. 

The result of the slop chart shows that there is not much change between the two years for the arrest rate of race, borough, and gender, while there is an obvious difference for the age group and the offense description.  
```{r question 1 slope chart 2015 vs 2018}

# Race
rate.by.race = clean_dat[yr >= 2015, .N, by = c('yr','perp_race')]
rate.by.race[, rate := round((N / total.pop)*100, digits = 2), by = yr]
rate.by.race[, perc := round(rate / sum(rate), digits = 2), by = yr]
setorderv(rate.by.race, cols = c('yr', 'perp_race'), order = c(-1,1))
setnames(x = rate.by.race, old = 'perp_race', new = 'target')

create.slop.chart(dat = rate.by.race, title = 'Race')

# age
rate.by.age = clean_dat[yr >= 2015, .N, by = c('yr','age_group')]
rate.by.age[, rate := round((N / total.pop)*100, digits = 2), by = yr]
rate.by.age[, perc := round(rate / sum(rate), digits = 2), by = yr]
setorderv(rate.by.age, cols = c('yr', 'age_group'), order = c(-1,1))
setnames(x = rate.by.age, old = 'age_group', new = 'target')

create.slop.chart(rate.by.age, 'Age')

# borough
rate.by.borough = clean_dat[yr >= 2015, .N, by = c('yr','arrest_boro')]
rate.by.borough[, rate := round((N / total.pop)*100, digits = 2), by = yr]
rate.by.borough[, perc := round(rate / sum(rate), digits = 2), by = yr]
setorderv(rate.by.borough, cols = c('yr', 'arrest_boro'), order = c(-1,1))
setnames(x = rate.by.borough, old = 'arrest_boro', new = 'target')

create.slop.chart(dat = rate.by.borough, 'Borough')

# sex
rate.by.sex = clean_dat[yr >= 2015, .N, by = c('yr','perp_sex')]
rate.by.sex[, rate := round((N / total.pop)*100, digits = 2), by = yr]
rate.by.sex[, perc := round(rate / sum(rate), digits = 2), by = yr]
setorderv(rate.by.sex, cols = c('yr', 'perp_sex'), order = c(-1,1))
setnames(x = rate.by.sex, old = 'perp_sex', new = 'target')

create.slop.chart(dat = rate.by.sex, 'Gender')

# offense description
rate.by.ofns_desc = clean_dat[yr >= 2015, .N, by = c('yr','ofns_desc')]
rate.by.ofns_desc[, rate := round((N / total.pop)*100, digits = 2), by = yr]
rate.by.ofns_desc[, perc := round(rate / sum(rate), digits = 2), by = yr]
setorderv(rate.by.ofns_desc, cols = c('yr', 'perc'), order = c(-1,-1))
setnames(x = rate.by.ofns_desc, old = 'ofns_desc', new = 'target')

# take the top 5
rate.by.ofns_desc.top5 = rate.by.ofns_desc[, .SD[1:5] , by = yr]


create.slop.chart(dat = rate.by.ofns_desc.top5, 'Top5 Types')
```


##### Demographic & Top 5 Types
+ Methodology: calculate the percentage of each factor of the variables: race, age group, gender, borough, and top 5 types of crime. Then, plot out the histogram of the account rates of each factor throughout the year.

Unlike the trend analysis on the overall arrest rate, the arrest rates based on the demographics do not show distinctive trends but stays almost the same. 
Still, one could see a change from the arrest rates of age group, borough, and top 5 offense descriptions: the arrest rate of the age group 25-44 increases, while that of 18-24 decreases; and, the assault 3 & related offenses and the vehicle and traffic laws increase as the times goes by, while the dangerous drugs and the other offenses related to theft are decreasing. The most interesting point from this plot would be that felony assault is raised in 2018 as one of the top 5 arrest types when other offenses related to theft is not on the list anymore.

Based on both charts, I could say that the arrest rates of the race and sex do not have any distinctive trends unlike those of the age group, borough, and the types of arrest.


  
```{r question 1 histogrm by yr}
# by race
create.histogram(dat = rate.by.race, title = 'Race')

# by age
create.histogram(dat = rate.by.age, title = 'Age')

# by borough
create.histogram(dat = rate.by.borough, title = 'Borough')

# by sex
create.histogram(dat = rate.by.sex, title = 'Gender')

# by ofns_desc
create.histogram(dat = rate.by.ofns_desc.top5, title = 'Top5 Types')

rm(rate.by.age)
rm(rate.by.borough)
rm(rate.by.ofns_desc)
rm(rate.by.race)
rm(rate.by.sex)
```


##### Answer
+ Methodology: Regression Analysis (linear regression)

<font size="3"> In short, the overall arrest rate throughout the time shows the **decreasing trend**; however, the arrest rate of race, gender, age group, borough, and the top 5 arrest types shows the different results: arrest rates of race, borough, and gender do not change much, while the top 5 arrest types and age group display distinctive changes. 

As an extension of descriptive statistics, I performed a multivariate linear regression with the following formula: 
$$  Rate =  b_1AgeGroup + b_2Race + b_3Gender + b_4Borough+ b_5Types + b_6Year $$

The result of the linear regression also states that the arrest rates are **decreasing** across the time, holding the above-mentioned variable fixed. Controlling for other variables, one year increase in a year would decrease the arrest rate by **-8.014e-07** on average. </font>

```{r question 1 regression analsis}

q1.dat = clean_dat[yr >=2015 & ofns_desc %in% unique(rate.by.ofns_desc.top5$target), .N, by = c('arrest_date', 'age_group', 'perp_race', 'perp_sex', 'arrest_boro', 'ofns_desc', 'yr', 'yr_mon')] 

rm(rate.by.ofns_desc.top5)

q1.dat[, Rate:= (N/total.pop)*100, by = yr]

formula = create.formula(outcome.name = 'Rate', input.names = c('age_group', 'perp_race', 'perp_sex', 'arrest_boro', 'ofns_desc', 'yr'), dat = q1.dat, max.input.categories = 7, reduce = T)

lm = lm(formula = formula, data = q1.dat)
summary(lm)
rm(q1.dat)

```


```{r space 1}

```

#### **2. What are the top 5 most frequent arrests as described in the column 'pd_desc' in 2018? Compare & describe the overall trends of these arrests across time.** {.tabset}

I break the question into two parts: figuring out the top 5 pd_desc in 2018 and looking for the trend. 
**Here, the question asked about the frequency, which I take it as the number of occurrences, instead of rate. That is, the following analysis is based on the absolute number rather than the rate, calculated by frequency/population**

##### Top 5 Types

There are 351 types of arrests in the column. Yet, there are 304 numbers of arrest types.

As Table 8 shows, the top 5 pd_desc types in 2018 are **Assult 3, Larceny Petit From Open Areas, Traffic, Assault 2&1, and Controlled Substance**.


```{r question 2 figuring out the top 5}
pd_desc.num = length(unique(clean_dat$pd_desc))
pd_desc.uniq = unique(clean_dat$pd_desc)

tab = clean_dat[yr == 2018, .N, by = pd_desc][order(-N)]
tab[, perc := round(N / sum(N) *100, digits = 2)]

datatable(tab[1:5], rownames = F, caption = 'Table 8.')
```


##### Monthly Top 5 (Additional) 
I wanted to check if the *monthly* top 5 types consistent with the *year* based. Table 9 shows that the yearly top 5 arrest type in 2018 **aligns with** that of monthly top 5 types of that year.
```{r question 2 figuriout out the top 5 monthly}
tab.mon = clean_dat[yr == 2018, .N, by = c('pd_desc','yr_mon' )]
tab.mon[, perc := round(N / sum(N) *100, digits = 2), by = 'yr_mon']
setorderv(tab.mon, cols = c('yr_mon', 'perc'), order = c(-1,-1))

tab.mon.top.5 = tab.mon[, .SD[1:5], by = yr_mon]

datatable(tab.mon.top.5,rownames = F)
datatable(tab.mon.top.5[, .N, by = pd_desc], caption = 'Table 9.')

rm(tab.mon.top.5)
```




##### Trend Analysis

Although the arrest frequency of Assault 3 has been varied between 29% and 43%, it accounts for the biggest portion of the arrests across the year, from 2009 to 2018. The frequency of another type of Assault, 2&1, does not show much fluctuation. Still, one could see that since 2012, it is not listed on the top 3 anymore. 

On the other hand, the frequency of the Controlled Substance, Possession 7 type is decreasing from 39% to 12%. Until 2015, it was rated as top 3 arrest types; nevertheless, it has made its progress that it is enlisted on bottom 2 since then. 

Starting from 2014, the arrest of crime type, LARCENY, PETIT FROM OPEN AREAS, UNCLASSIFIED, increased by 115.7584918 % in 2014 compared to the previous year. And, the rate remains that high or increases. Similarly, one could see that the arrest type of TRAFFIC, UNCLASSIFIED MISDEMEAN has a huge increase in 2012. In detail, it increased by 384.9508197 % compare to the previous year. And, it accounts for about 0.1714286% of the arrest since then.


```{r question 2 compare and trend}
top.5.pd_desc = tab[, pd_desc][1:5]

# parsing the top 5 from the overall data
comparison.tab = clean_dat[pd_desc %in% top.5.pd_desc, .N, by = c('yr','pd_desc')]

comparison.tab[, perc := round(N / sum(N), digits = 2), by = yr]
setorderv(comparison.tab, cols = c('yr', 'perc'), order = c(-1,-1))
comparison.tab[, rank:= 1:.N, by = yr]
comparison.tab[, diff:= rank - shift(rank), by = pd_desc]
datatable(comparison.tab, rownames = F, caption = 'Table 10.')

setnames(x = comparison.tab, old = 'pd_desc', new = 'target')

create.histogram(dat = comparison.tab, title = 'Top 5 Types', show.labels = F)

## overal time series
top.5.dat = clean_dat[pd_desc %in% top.5.pd_desc, .N, by = c( 'yr_mon')]
#top.5.dat = top.5.dat[, mean:= mean(N), by = c('yr_mon')]

top.ts = ts(data = top.5.dat[,-1], start = c(2006,1), end = c(2018,12), frequency = 12)

autoplot(top.ts, facet = T)+ ylab('Numb. of Arrest of Top 5')

ggAcf(x = top.ts, lag.max = 12*12)
pacf(top.ts)
# it is highly auto-correlated with the adjecent year and the year before with high seasonality

# STL (Seasonality and Trend Decomposition using Loess)
autoplot(stl(top.ts[,1], s.window = 'periodic', robust = T))


# Chekcing for the seaonality
ggseasonplot(top.ts, year.labels = TRUE, year.labels.left = TRUE) +
  ylab("Numb. of Top 5 Arrests") +
  xlab("Month") +
  ggtitle("Seasonal Plot: Top 5 Arrests") 

# Variation by the month
ggsubseriesplot(top.ts) +
  ylab("Numb. of Top 5 Arrests") +
  xlab("Month") +
  ggtitle("Sub Seasonal Plot: Top 5 Arrests")

boxplot(top.ts~cycle(top.ts), xlab = 'Months', ylab = 'Numb. of Top 5 Arrests')
```


##### Answer

<font size="3">The most frequent pd_desc types in 2018 are **Assult 3, Larceny Petit From Open Areas, Traffic, Assault 2&1, and Controlled Substance**. 

Regarding the overall trend of the top 5 arrests, one could say that the trend is **moving from increasing to decreasing starting from 2014 and that there is a high seasonality**. Lastly, December is the month that has the lowest arrest rates throughout the years, when May has the highest. 

Still, one could say that the **frequency** of those top 5 types is **increased** due to the fact that the mean of the overall number of arrests of those types is increased compared to that of the beginning year. This could be supported by the plot as followed:</font>

```{r question 2 regession analysis}
q2.dat = clean_dat[pd_desc %in% top.5.pd_desc, .N, by = c('yr','pd_desc', 'arrest_date', 'yr_mon')]

q2.dat[, mean:= mean(N), by = yr_mon]

#formula2 = create.formula(outcome.name = 'mean', input.names = c('yr', 'pd_desc'), dat = q2.dat)

#lm2 = lm(formula = formula2, data = q2.dat)
#summary(lm2)

ggplot(q2.dat, aes(x = yr, y = mean)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

rm(q2.dat)
```


```{r space 2}

```


#### **3. If we think of arrests as a sample of total crime, is there more crime in precinct 19 (Upper East Side) than precinct 73 (Brownsville)? Describe the trend, variability and justify any statistical tests used to support this conclusion.** {.tabset}

The question mentioned that the data, sample, could represent the total crime. That is, it is a representative sample that allows me to draw a conclusion of the population based on the analysis of the sample with confidence. Say that, number of arrests can represent the crime rates. With that in mind, the analysis is conducted. 

**As a side note: The overall crime rate in Brownsville, precinct 73, is 16% higher than the national average, while the overall crime rate in Upper East Side, precinct 19, is 28% lower than the national average. (reference: https://www.areavibes.com/brownsville-tx/crime/)**

##### 19>73?
+ Methodology: calculate the **monthly mean of arrests** of each precinct across the year, then take the **difference** between them. And, plot out the difference by year. Here, I took the mean difference to alleviate the difference between the two districts as well as the difference across the time.

Answering the first part of the question, precinct 73 has a **greater average amount of crime** than precinct 19 across the time on average. And, this aligns with the aforementioned reference. It claims that precinct 73 has higher crime rates than the national average, while precinct 19 has fewer crime rats than that.
```{r question 3}
# parsing 19
tab.19 = clean_dat[arrest_precinct == 19, .N, by = c('arrest_date','arrest_precinct', 'yr_mon')]
# compute the arrest rate by mean of month
tab.19 = tab.19[, mean(N), by= yr_mon]
setnames(x = tab.19, old = 'V1', new = '19')

# parsing 73
tab.73 = clean_dat[arrest_precinct == 73, .N, by = c('arrest_date','arrest_precinct','yr_mon')]
# compute the arrest rate by mean of month
tab.73 = tab.73[, mean(N), by= yr_mon]
setnames(x = tab.73, old = 'V1', new = '73')

tab.19.73 = merge(tab.19, tab.73, by = 'yr_mon')

#basic.stat(tab.19.73$`19`)
#basic.stat(tab.19.73$`73`)
#cor(tab.19.73$`19`, tab.19.73$`73`)

tab.19.73[, mean_diff := `73` - `19`, by = yr_mon]

ggplot(tab.19.73, aes(x=as.Date(paste0(yr_mon,"-01")), y=mean_diff)) +
  geom_line(color = '#69b3a2')+
  xlab("") +
  ylab("Difference over the time (73 - 19)")+
  scale_x_date(date_breaks = "6 month", date_labels = "%y")

```


##### Trend Analysis

Regarding the trend, one could say that it is **moving from increasing to decreasing trend from 2011 with seasonality**. 

NOTE: The trend is based on the difference in the monthly mean arrest between the two regions across the year.

```{r question 3 time series}

final.tab = tab.19.73[, c(1,4)]

ts.dat = ts(data = final.tab[,-1], start = c(2006,1), end = c(2018,12), frequency = 12)

plot.ts(ts.dat, type= "b", main = "Mean Difference Over Time", xlab = "Years", ylab = "Difference btw the Precincts", col = '#69b3a2', pch = "*") 

# STL (Seasonality and Trend Decomposition using Loess)
autoplot(stl(ts.dat[,1], s.window = 'periodic'))

# Chekcing for the seaonality
ggseasonplot(ts.dat, year.labels = TRUE, year.labels.left = TRUE) +
  ylab("Difference btw the Precincts") +
  xlab("") +
  ggtitle("Seasonal Plot: Difference Over Time") # Weak Seaonality

# Variation by the month
ggsubseriesplot(ts.dat) +
  ylab("Difference btw the Precincts") +
  xlab("") +
  ggtitle("Sub Seasonal Plot: Difference Over Time")

boxplot(ts.dat~cycle(ts.dat), xlab = 'Months', ylab = 'Difference btw the Precincts')
```


##### Answer
<font size="3"> Precinct 73 has a **greater average amount of crime** than precinct 19 across the time on average.
Yet, to answer this question, one should be careful since the analysis disregard the demographic difference between the two communities. To confirm, I ran a linear regression. 

Based on the P-val of the precinct, one could say that the **location** does **not** have a statistical impact on the mean of crime controlling the year fixed. In contrast, the year has a statistical impact on the monthly mean number of arrests. 

Regarding the trend, one could say that it is **moving from increasing to decreasing trend from 2011 with seasonality**. This happens mostly because the crime rates in precinct 73 are decreasing. For instance, after reaching the peak rate, 43.87%, in 2011-07, it gradually decreases. And, the rate remains around 13% since 2018-08. 

*Further research on the characteristics of each neighborhood such as ethnicity, environment, economic status, and such could help one to have a better understanding of the crime rates.*
</font>
```{r question3 regression analysis}
q3.dat = clean_dat[arrest_precinct == 19 | arrest_precinct == 73, .N, by = c('yr', 'arrest_date', 'yr_mon', 'arrest_precinct')]

q3.dat[, mean := mean(N), by = yr_mon]
q3.dat$arrest_precinct = as.factor(q3.dat$arrest_precinct)

formula3 = create.formula(outcome.name = 'mean', input.names = c('yr', 'arrest_precinct'), dat = q3.dat)

lm3 = lm(formula = formula3, data = q3.dat)
summary(lm3)

ggplot(q3.dat, aes(x = yr, y = mean)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
rm(q3.dat)
```

```{r plot for 19&73}
ggplot(tab.19.73, aes(x=as.Date(paste0(yr_mon,"-01")), y=`73`)) +
  geom_line(color = 'red')+
  xlab("") +
  ylab("Mean Crimes of '73'")+
  scale_x_date(date_breaks = "6 month", date_labels = "%y")
```


```{r space 3}

```


#### **4. What model would you build to predict crime to better allocate NYPD resources? What challenges do you foresee? What variables would be included? How would you evaluate the model? Discuss in no more than 150 words.**{.tabset}

##### Validation

As a stationary check, I ran the Dickey-Fuller Test (adf.test), Kwiatkowski-Phillips-Schmidt-Shin (kpss.test), and the Ljung-Box test(Box.test). According to the p-values of `Box.test` and `kpss.test`, it is a random walk; however, `adf.test` says otherwise: the data is stationary. The result of `adf.test` contradicts that of `kpss.test`. It may be caused because I do not have enough data or neither test is powerful enough to reject the null hypothesis. 
**Here, the rate is not stabilized, which could be done with Box-Cox Transformation as followed: `BoxCox(ts.dat,lambda = BoxCox.lambda(ts.dat))`. Also, the trend and seasonality are not removed yet**

```{r question 4 data validation}
# Stationary
ggAcf(x = ts.dat, lag.max = 12*12)
pacf(ts.dat)

# Random Walk test:
Box.test(ts.dat, type = "Ljung-Box", lag = 12*12) # NOT WHITE NOISE

# Stationary test restrict one!
adf.test(ts.dat, alternative="stationary", k = 0) # p = 0.01. That is, it is stationary.
# Stationary around trend test
kpss.test(ts.dat, null = 'Trend', lshort = T) # p-val = 0.01, The null hypothesis of stationarity around a trend is rejected.

# White noise around trend test
kpss.test(ts.dat, null = 'Level', lshort = T) # p-val = 0.01:  The null hypothesis of stationarity around a level is rejected. 
```



##### Additive vs.Multiplicative

The result says it is *multiplicative* model. 


```{r question 4 figuring out Additive or Multiplicative for ets}
arrest.dat[, trend:= rollmean(x = Rate, k = 12, fill=NA, align = "right")]

arrest.dat[,`:=`(detrended_add = Rate - trend,  detrended_mult = Rate / trend )]

arrest.dat[, yr_mon:= paste0(yr_mon,"-01")]
arrest.dat$yr_mon = as.Date(arrest.dat$yr_mon)

arrest.dat[,`:=`
           (seasonal_add = mean(detrended_add, na.rm = TRUE),
            seasonal_mult = mean(detrended_mult, na.rm = TRUE)), by=.(quarter(yr_mon))]

arrest.dat[,`:=`
           (residual_add = detrended_add - seasonal_add,
            residual_mult = detrended_mult / seasonal_mult )]

arrest.dat[,.(ts_type = compare_ssacf(residual_add, residual_mult))] # Multiplicative

```

##### Models

This tab provides the result of a variety of models. And, based on the AIC, RMSE, ME, and MPE, ARIMA(0,1,1)(0,1,1)[12] provides the best forecast. Here, `Train data contains records from 2006 to 2014, and Test data is from 2015 to 2018.`


```{r question 4 finding the best model}
# split the data into train and test:
train = window(ts.dat, end = c(2014, 12))
test = window(ts.dat, start = c(2015, 01))
length(train)
length(test)

## Avg Method:
average_model = meanf(train,h = 48)
accuracy(average_model,x = ts.dat)

## Naive Method:
naive_model = naive(train,h=48)
accuracy(naive_model,x = ts.dat)

## Seasonal Naive Method:
seasonal_naive_model = snaive(train,h=48)
accuracy(seasonal_naive_model,x = ts.dat)

## Plots 1:
autoplot(train)+
  autolayer(average_model,PI = F,size=1.1,series = 'Average Model')+
  autolayer(naive_model,PI=F,size=1.1, series='Naive Model')+
  autolayer(seasonal_naive_model,PI=F,size=1.1,series='Seasonal Naive Model')+
  autolayer(test)

## Drift Method:
drift_model = rwf(train,h=48,drift = T)
accuracy(drift_model,x = ts.dat)


## Holt's Method with Damping:
holt_damped_model = holt(train,h=48,damped = T)
accuracy(holt_damped_model,x = ts.dat)

## Holt-Winter's Seasonal Method with Multiplicative:
hw_mult = hw(train,h=48,seasonal = 'multiplicative', damped=T)
accuracy(hw_mult,x = ts.dat)

## Plots 2:
autoplot(train)+
  autolayer(drift_model,PI=F,size=1.1,series='Drift Model')+
  autolayer(holt_damped_model,series="Holt's Method with Damping",PI=F,size=1.1)+
  autolayer(hw_mult,series="Holt Winter's Method - Multiplicative",PI=F)+
  autolayer(test)


model.types = c("MAM","ANA","MNM","MAM","MMM")
ets.models = list()
ets.forecast = list()
for(i in seq(model.types)){
  ets.models[[i]] = ets(train, model =  model.types[i])
  summary(ets.models[[i]])
  
  ets.forecast[[i]] = forecast(ets.models[[i]],h=48)
  
  accuracy(ets.forecast[[i]],x = ts.dat)
}


## Exponential Smoothing Methods(ETS)-xxx and auto:
ets_mmm = ets(train, model = 'MMM')
summary(ets_mmm)

ets_auto = ets(train)
summary(ets_auto) # ANA

ets_mmm_forecast = forecast(ets_mmm,h=48)
ets_auto_forecast = forecast(ets_auto,h=48)

accuracy(ets_mmm_forecast,x = ts.dat)
accuracy(ets_auto_forecast,x = ts.dat) 

## Plots 3:
autoplot(train)+
  autolayer(ets_mmm_forecast,size = 1.1, series="ETS - MMM",PI=F)+
  autolayer(ets_auto_forecast,size = 1.1, series="ETS - AUTO",PI=F)+
  autolayer(test)


## ARIMA: 

### Stablizing:
train2 = BoxCox(train,lambda = BoxCox.lambda(train))
optimal.lamda = BoxCox.lambda(train)
kpss.test(train2) # Still non-staionary

### Removing Seasonality:
train2_no_seasonality = diff(x = train2,lag = 12)
kpss.test(train2_no_seasonality) # Still non-staionary
ndiffs(train2_no_seasonality) #1

### Removing Trend:
train2_no_season_no_trend = diff(train2_no_seasonality,1)
autoplot(train2_no_season_no_trend)
kpss.test(train2_no_season_no_trend) # Now stationary!!


plot = 
    cbind(
      original = train,
      const_var = train2,
      no_seasonality = train2_no_seasonality,
      no_seasonality_trend = train2_no_season_no_trend)

autoplot(plot,facets=T,colour = T)+ylab('')+xlab('Year')+theme_bw()


### Examine ACF and PACF to decide on AR (ARIMA(p,d,0) and/or MA terms (ARIMA(0,d,q))
train2_no_season_no_trend %>%
  diff(lag=12)%>%
  diff(lag=1)%>%
  ggtsdisplay()


ggAcf(train2_no_season_no_trend, lag.max = 12*12)# First lag is NEGATIVE: MA model. That is, ARIMA(0, d, q)
pacf(train2_no_season_no_trend, lag.max = 12*12)# bit gradually decrease than acf 


### Try chose a model by selecing the min AICc:
arima.model1 = Arima(y = train,order = c(0,1,2),seasonal = c(0,1,1),lambda = BoxCox.lambda(train))
arima.model1

ggtsdisplay(residuals(arima.model1))
checkresiduals(arima.model1)## NOT FIT YET!

autoplot(forecast(arima.model1,h = 48),PI=F)+
  autolayer(test,size=1)

### Alternative: 
arima.model2 = Arima(y = train, order = c(0,1,1),seasonal = c(0,1,1),lambda = BoxCox.lambda(train))
arima.model2

ggtsdisplay(residuals(arima.model2))
checkresiduals(arima.model2)## p > 0.05: FIT!!

autoplot(forecast(arima.model2,h = 48),PI=F)+
  autolayer(test,size=1)

### AUTO:
arima_auto = auto.arima(y = train, stepwise = F,approximation = F)
arima_auto

autoplot(forecast(arima_auto,h = 48),PI=F)+
  autolayer(test,size=1)


## Accuracy:
arima.model1_forecast = forecast(arima.model1,h=48)
arima.model2_forecast = forecast(arima.model2,h=48) 
arima.auto_forecast = forecast(arima_auto, h= 48)
### arima.model1 is better?

accuracy(arima.model1_forecast,x = ts.dat) 
accuracy(arima.model2_forecast,x = ts.dat) 
accuracy(arima.auto_forecast,x = ts.dat) 


## Plots 4:
autoplot(train)+
  autolayer(arima.model1_forecast,series="ARIMA - 1",PI=F, size = 1.1)+
  autolayer(arima.model2_forecast,series="ARIMA - 2",PI=F, size = 1.1)+
  autolayer(arima.auto_forecast,series="ARIMA - AUTO",PI=F, size = 1.1)+
  autolayer(test)


## Resulting Tab for the model accuracy
reporting.tab = 
  rbind(
      average_model = accuracy(f = average_model,x = ts.dat)[2,],
      naive_model = accuracy(f = naive_model,x = ts.dat)[2,],
      seasonal_naive_model = accuracy(f = seasonal_naive_model,x = ts.dat)[2,],
      drift_model = accuracy(f = drift_model, x = ts.dat)[2,],
      holt_damped_model = accuracy(f = holt_damped_model,x = ts.dat)[2,],
      hw_mult = accuracy(f = hw_mult, x = ts.dat)[2,],
      ets_mmm = accuracy(ets_mmm_forecast, x = ts.dat)[2,],
      ets_auto = accuracy(ets_auto_forecast, x = ts.dat)[2,],
      arima.model1 = accuracy(arima.model1_forecast, x = ts.dat)[2,],
      arima.model2 = accuracy(arima.model1_forecast, x = ts.dat)[2,],
      arima.auto = accuracy(arima.auto_forecast,x=ts.dat)[2,]
      )

datatable(reporting.tab[,c("RMSE", 'ME', 'MPE')], class = 'cell-border stripe', caption = 'Table 11.')

```

##### Answer
<font size="3"> Forecasting, in general, is difficult, especially when the outcome is expected to be influenced by unexpected variables. Hence, to select a model, I generated various models. And, based on the AIC, RMSE, ME, and MPE of the multiple models, I would utilize the *ARIMA(0,1,1)(0,1,1)[12]* model to forecast crime.

The analysis is based on the arrest rate based on *uniformed total population* across the year. As the above analysis shows, this tends to ignore the difference among the districts, race, age group, borough, crime types and such. 

Incorporating the variables that could *explain the characteristics of each district* - race, age group, borough, and crime types -  would be helpful. Still, this is *vulnerable to bias*. 

As an evaluation, I would pay attention to *RMSE, ME, and MPE more* because large errors are undesirable in terms of forecasting, and ME and MPE allow for the bias check. </font>
